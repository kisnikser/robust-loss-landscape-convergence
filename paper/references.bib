@inproceedings{choromanska2015losssurfacesmultilayernetworks,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial intelligence and statistics},
  pages={192--204},
  year={2015},
  organization={PMLR}
}


@article{garipov2018losssurfacesmodeconnectivity,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@inproceedings{draxler2019essentiallynobarriers,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}


@inproceedings{nguyen2017losssurfacedeepwide,
  title={The loss surface of deep and wide neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2603--2612},
  year={2017},
  organization={PMLR}
}


@article{hochreiter1997flatminima,
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal = {Neural Computation},
	month = {01},
	pages = {1-42},
	title = {Flat Minima},
	volume = {9},
	year = {1997}}


@inproceedings{dinh2017sharpminimageneralizedeep,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}


@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{singh2022phenomenologydoubledescentfinitewidth,
  title={Phenomenology of double descent in finite-width neural networks},
  author={Singh, Sidak Pal and Lucchi, Aurelien and Hofmann, Thomas and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2203.07337},
  year={2022}
}


@article{wang2023instabilitieslargelearningrate,
  title={The instabilities of large learning rate training: a loss landscape view},
  author={Wang, Lawrence and Roberts, Stephen},
  journal={arXiv preprint arXiv:2307.11948},
  year={2023}
}


@article{singh2024landscapinglinearmodeconnectivity,
  title={Landscaping linear mode connectivity},
  author={Singh, Sidak Pal and Adilova, Linara and Kamp, Michael and Fischer, Asja and Sch{\"o}lkopf, Bernhard and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2406.16300},
  year={2024}
}


@article{chen2022visiontransformersoutperformresnets,
  title={When vision transformers outperform resnets without pre-training or strong data augmentations},
  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  journal={arXiv preprint arXiv:2106.01548},
  year={2021}
}


@article{lee2024visualizinglosslandscapeselfsupervised,
  title={Visualizing the loss landscape of Self-supervised Vision Transformer},
  author={Lee, Youngwan and Willette, Jeffrey Ryan and Kim, Jonghee and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2405.18042},
  year={2024}
}


@article{sagun2017eigenvalueshessiandeeplearning,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}


@inproceedings{ghorbani2019investigation,
  title={An investigation into neural net optimization via hessian eigenvalue density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  booktitle={International Conference on Machine Learning},
  pages={2232--2241},
  year={2019},
  organization={PMLR}
}


@article{papyan2019spectrumdeepnethessiansscale,
  title={The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}


@article{papyan2020tracesclasscrossclassstructurepervade,
  title={Traces of class/cross-class structure pervade deep learning spectra},
  author={Papyan, Vardan},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={252},
  pages={1--64},
  year={2020}
}


@article{papyan2019measurementsthreelevelhierarchicalstructure,
  title={Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1901.08244},
  year={2019}
}


@article{liao2021hessianeigenspectrarealisticnonlinear,
  title={Hessian eigenspectra of more realistic nonlinear models},
  author={Liao, Zhenyu and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20104--20117},
  year={2021}
}


@article{xie2022powerlawhessianspectrumsdeep,
  title={On the power-law hessian spectrums in deep learning},
  author={Xie, Zeke and Tang, Qian-Yuan and Cai, Yunfeng and Sun, Mingming and Li, Ping},
  journal={arXiv preprint arXiv:2201.13011},
  year={2022}
}


@article{pmlr-v70-pennington17a,
  title={Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{wu2022dissectinghessianunderstandingcommon,
  title={Dissecting hessian: Understanding common structure of hessian in neural networks},
  author={Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie and Ge, Rong},
  journal={arXiv preprint arXiv:2010.04261},
  year={2020}
}


@article{singh2021analyticinsightsstructurerank,
  title={Analytic insights into structure and rank of neural network hessian maps},
  author={Singh, Sidak Pal and Bachmann, Gregor and Hofmann, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23914--23927},
  year={2021}
}


@article{singh2023hessianperspectivenatureconvolutional,
  title={The hessian perspective into the nature of convolutional neural networks},
  author={Singh, Sidak Pal and Hofmann, Thomas and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2305.09088},
  year={2023}
}


@article{skorski2019chainruleshessianhigher,
  title={Chain rules for hessian and higher derivatives made easy by tensor calculus},
  author={Skorski, Maciej},
  journal={arXiv preprint arXiv:1911.13292},
  year={2019}
}


@article{fort2019emergentpropertieslocalgeometry,
  title={Emergent properties of the local geometry of neural loss landscapes},
  author={Fort, Stanislav and Ganguli, Surya},
  journal={arXiv preprint arXiv:1910.05929},
  year={2019}
}


@inproceedings{ju2023robustfinetuningdeepneural,
  title={Robust fine-tuning of deep neural networks with hessian-based generalization guarantees},
  author={Ju, Haotian and Li, Dongyue and Zhang, Hongyang R},
  booktitle={International conference on machine learning},
  pages={10431--10461},
  year={2022},
  organization={PMLR}
}


@article{nguyen2024agnosticsharpnessawareminimization,
  title={Agnostic Sharpness-Aware Minimization},
  author={Nguyen, Van-Anh and Tran, Quyen and Truong, Tuan and Do, Thanh-Toan and Phung, Dinh and Le, Trung},
  journal={arXiv preprint arXiv:2406.07107},
  year={2024}
}


@article{macdonald2023progressivesharpeningflatminima,
  title={On progressive sharpening, flat minima and generalisation},
  author={MacDonald, Lachlan Ewen and Valmadre, Jack and Lucey, Simon},
  journal={arXiv preprint arXiv:2305.14683},
  year={2023}
}


@misc{wu2017towards,
      title={Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes}, 
      author={Lei Wu and Zhanxing Zhu and Weinan E},
      year={2017},
      eprint={1706.10239},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.10239}, 
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}


@article{bousquet2002stability,
  title={Stability and Generalization},
  author={Bousquet, Olivier and Elisseeff, Andr√©},
  journal={Journal of Machine Learning Research},
  year={2002}
}


@article{elisseeff2005stability,
  author  = {Andre Elisseeff and Theodoros Evgeniou and Massimiliano Pontil},
  title   = {Stability of Randomized Learning Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {3},
  pages   = {55--79},
}


@inproceedings{hardt2016trainingsgd,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}


@article{NEURIPS2018_5a4be1fa,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{Lee_2020,
  title={Finite versus infinite neural networks: an empirical study},
  author={Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15156--15172},
  year={2020}
}


@article{neyshabur2017exploringgeneralizationdeeplearning,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{kiselev2025likelihood,
  title={Sample Size Determination: Likelihood Bootstrapping},
  author={Kiselev, N. S. and Grabovoy, A. V.},
  journal={Computational Mathematics and Mathematical Physics},
  volume={65},
  number={3},
  pages={416--423},
  year={2025},
  doi={10.1134/S0965542524702002}
}


@article{kiselev2025posterior,
  title={Sample Size Determination: Posterior Distributions Proximity},
  author={Kiselev, N. S. and Grabovoy, A. V.},
  journal={Computational Management Science},
  volume={22},
  number={1},
  pages={1},
  year={2025},
  doi={10.1007/s10287-024-00528-9}
}


@article{kiselev2024unraveling,
  author = {Kiselev, N. S. and Grabovoy, A. V.},
  journal = {Doklady Mathematics},
  number = {1},
  pages = {S49--S61},
  title = {Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes},
  volume = {110},
  year = {2024}
}


@INPROCEEDINGS{meshkov2024conv,
  author={Meshkov, Vladislav and Kiselev, Nikita and Grabovoy, Andrey},
  booktitle={2024 Ivannikov Ispras Open Conference (ISPRAS)}, 
  title={ConvNets Landscape Convergence: Hessian-Based Analysis of Matricized Networks}, 
  year={2024},
  pages={1-10},
  doi={10.1109/ISPRAS64596.2024.10899113}
}



